{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea04a218",
   "metadata": {},
   "source": [
    "## Assignment 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d11ff2",
   "metadata": {},
   "source": [
    "### Part 1: Image Generation and Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142916d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import clip\n",
    "from utils import UNet_utils, ddpm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59580f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc7a3db",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(B_start, B_end, T)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m ddpm \u001b[38;5;241m=\u001b[39m ddpm_utils\u001b[38;5;241m.\u001b[39mDDPM(B, device)\n\u001b[0;32m---> 12\u001b[0m clip_model, clip_preprocess \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-B/32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m clip_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     14\u001b[0m CLIP_FEATURES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 32\n",
    "IMG_CH = 3\n",
    "BATCH_SIZE = 128\n",
    "INPUT_SIZE = (IMG_CH, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "T = 400\n",
    "B_start = 0.0001\n",
    "B_end = 0.02\n",
    "B = torch.linspace(B_start, B_end, T).to(device)\n",
    "ddpm = ddpm_utils.DDPM(B, device)\n",
    "\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\")\n",
    "clip_model.eval()\n",
    "CLIP_FEATURES = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b9059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the UNet model identical to the one in notebook 05.\n",
    "model = UNet_utils.UNet(\n",
    "    T, IMG_CH, IMG_SIZE, down_chs=(256, 256, 512), t_embed_dim=8, c_embed_dim=CLIP_FEATURES\n",
    ")\n",
    "\n",
    "# Load the pre-trained model weights\n",
    "model.load_state_dict(torch.load('path_to_your_model.pth'))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Text Prompts to genereate Images From\n",
    "text_prompts = [\n",
    "    \"A photo of a red rose\",\n",
    "    \"A photo of a white daisy\",\n",
    "    \"A photo of a yellow sunflower\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embedding Extraction using Hooks ---\n",
    "embeddings_storage = {}\n",
    "\n",
    "def get_embedding_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        embeddings_storage[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks to capture embeddings from the \"down2\" layer of the U-Net model\n",
    "model.down2.register_forward_hook(get_embedding_hook(\"down2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec4338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_flowers(text_list):\n",
    "    text_tokens = clip.tokenize(text_list).to(device)\n",
    "    c = clip_model.encode_text(text_tokens).float()\n",
    "    x_gen, x_gen_store = ddpm_utils.sample_w(model, ddpm, INPUT_SIZE, T, c, device)\n",
    "    return x_gen, x_gen_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images, _ = sample_flowers(text_prompts)\n",
    "\n",
    "extracted_embeddings = embeddings_storage['down2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1bce86",
   "metadata": {},
   "source": [
    "### Part 2: Evaluation with CLIP Score and Frechet Inception Distance (FID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df68c4c",
   "metadata": {},
   "source": [
    "#### Metric Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23beafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d66fae1",
   "metadata": {},
   "source": [
    "##### CLIP Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b2e0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clip_score(image_path, text_prompt):\n",
    "    # Load model\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "\n",
    "    # Preprocess inputs\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
    "\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    text = tokenizer([text_prompt])\n",
    "\n",
    "    # Compute features and similarity\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "        \n",
    "        # Normalize features\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Calculate dot product\n",
    "        score = (image_features @ text_features.T).item()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcce8cf",
   "metadata": {},
   "source": [
    "##### Frechet Inception Distance (FID) Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(real_embeddings, gen_embeddings):\n",
    "    # Calculate mean and covariance\n",
    "    mu1, sigma1 = real_embeddings.mean(axis=0), np.cov(real_embeddings,rowvar=False)\n",
    "    mu2, sigma2 = gen_embeddings.mean(axis=0), np.cov(gen_embeddings, rowvar=False)\n",
    "    \n",
    "    # Calculate sum squared difference between means\n",
    "    ssdiff = np.sum((mu1 - mu2)**2)\n",
    "\n",
    "    # Calculate sqrt of product of covariances\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "    # Handle numerical errors\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    # Final FID calculation\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac6f30",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'open_clip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopen_clip\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# TODO: Calculate the CLIP score for each generated image against its prompt.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# You can use the `calculate_clip_score` function from the evaluation guide.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Calculate CLIP scores\u001b[39;00m\n\u001b[1;32m     16\u001b[0m clip_scores \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'open_clip'"
     ]
    }
   ],
   "source": [
    "# Calculate CLIP scores\n",
    "clip_scores = []\n",
    "for i, img in enumerate(generated_images):\n",
    "    score = calculate_clip_score(clip_model, img.unsqueeze(0), text_prompts[i], device)\n",
    "    clip_scores.append(score)\n",
    "average_clip_score = sum(clip_scores) / len(clip_scores)\n",
    "print(f\"Average CLIP Score: {average_clip_score}\")\n",
    "\n",
    "# Calculate FID score\n",
    "# Load real TF-Flowers dataset images here (not shown)\n",
    "real_images = ...\n",
    "fid_score = calculate_fid(real_images, generated_images)\n",
    "print(f\"FID Score: {fid_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd852d6",
   "metadata": {},
   "source": [
    "### Part 3: Embedding Analysis with FiftyOne Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a279b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new FiftyOne dataset\n",
    "dataset = fo.Dataset(name=\"generated_flowers_with_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each image, create a fiftyone.Sample and add the following metadata:\n",
    "# - The file path to the saved image.\n",
    "# - The text prompt (as a `fo.Classification` label).\n",
    "# - The CLIP score (as a custom field).\n",
    "# - The extracted U-Net embedding (as a custom field).\n",
    "\n",
    "from matplotlib import transforms\n",
    "\n",
    "\n",
    "for i, img in enumerate(generated_images):\n",
    "    # Save image to disk\n",
    "    img_path = f\"generated_image_{i}.png\"\n",
    "    # Assuming img is a PIL Image or can be converted to one\n",
    "    img_pil = transforms.ToPILImage()(img.cpu())\n",
    "    img_pil.save(img_path)\n",
    "    \n",
    "    # Create sample\n",
    "    sample = fo.Sample(\n",
    "        filepath=img_path,\n",
    "        prompt=fo.Classification(label=text_prompts[i]),\n",
    "        clip_score=clip_scores[i],\n",
    "        unet_embedding=extracted_embeddings[i].cpu().numpy()\n",
    "    )\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "# Now that the dataset is populated, use FiftyOne Brain to analyze the embeddings.\n",
    "brain = fob.EmbeddingBrain()\n",
    "brain.compute_embeddings(dataset, \"unet_embedding\")\n",
    "view = brain.cluster()\n",
    "session = fo.launch_app(view=view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute uniqueness of the dataset based on U-Net embeddings\n",
    "fob.compute_uniqueness(dataset)\n",
    "\n",
    "# Compute representativeness of the dataset based on U-Net embeddings\n",
    "fob.compute_representativeness(dataset, embeddings=\"unet_embedding\")\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65a9b7",
   "metadata": {},
   "source": [
    "### Part 4: Logging with Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df62c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"diffusion_model_assessment_v2\")\n",
    "\n",
    "# Log your hyperparameters (e.g., guidance weight `w`, number of steps `T`).\n",
    "wandb.config.update({\n",
    "    \"guidance_weight\": w,\n",
    "    \"num_steps\": T,\n",
    "})\n",
    "\n",
    "# Log evaluation metrics (CLIP Score and FID).\n",
    "wandb.log({\n",
    "    \"average_clip_score\": average_clip_score,\n",
    "    \"fid_score\": fid_score,\n",
    "})\n",
    "\n",
    "# Create wandb Table and log results\n",
    "table = wandb.Table(columns=[\"image\", \"prompt\", \"clip_score\", \"uniqueness\", \"representativeness\"])\n",
    "for sample in dataset:\n",
    "    img = wandb.Image(sample.filepath)\n",
    "    prompt = sample.prompt.label\n",
    "    clip_score = sample.clip_score\n",
    "    uniqueness = sample.metadata[\"fiftyone\"][\"uniqueness\"]\n",
    "    representativeness = sample.metadata[\"fiftyone\"][\"representativeness\"]\n",
    "    \n",
    "    table.add_data(img, prompt, clip_score, uniqueness, representativeness)\n",
    "wandb.log({\"generated_flowers_table\": table})\n",
    "\n",
    "# Finish the wandb run\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahocv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

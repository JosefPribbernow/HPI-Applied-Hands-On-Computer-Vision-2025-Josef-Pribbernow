{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73275a49",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60ab08",
   "metadata": {},
   "source": [
    "# 5. Assessment with WANDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890fdc5d",
   "metadata": {},
   "source": [
    "Congratulations on going through today's course! Hope it was a fun journey with some new skills as souvenirs. Now it's time to put those skills to the test.\n",
    "\n",
    "Here's the challenge: Let's say we have a have a classification model that uses LiDAR data to classify spheres and cubes. Compared to RGB cameras, LiDAR sensors are not as easy to come by, so we'd like to convert this model so it can classify RGB images instead. Since we used [NVIDIA Omniverse](https://www.nvidia.com/en-us/omniverse/) to generate LiDAR and RGB data pairs, let's use this data to create a contrastive pre-training model. Since CLIP is already taken, we will call this model `CILP` for \"Contrastive Image LiDAR Pre-training\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1083571",
   "metadata": {},
   "source": [
    "## 5.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb74e7",
   "metadata": {},
   "source": [
    "Let's get started. Below are the libraries used in this assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c241ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjosefpribbernow\u001b[0m (\u001b[33mjosefpribbernow-hasso-plattner-institute\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from assessment import assessment_utils\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30f447-41fd-4b60-9e11-d325ca379bb7",
   "metadata": {},
   "source": [
    "### 5.1.1 The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d69e9d3",
   "metadata": {},
   "source": [
    "Next, let's load our classification model and call it `lidar_cnn`. If we take a moment to view the [assement_utils](assessment/assesment_utils.py), we can see the `Classifier` class used to construct the model. Please note the `get_embs` method, which we will be using to construct our cross-modal projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cf3584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (embedder): Sequential(\n",
       "    (0): Conv2d(1, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=3200, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lidar_cnn = assessment_utils.Classifier(1).to(device)\n",
    "lidar_cnn.load_state_dict(torch.load(\"assessment/lidar_cnn.pt\", weights_only=True))\n",
    "# Do not unfreeze. Otherwise, it would be difficult to pass the assessment.\n",
    "for param in lidar_cnn.parameters():\n",
    "    lidar_cnn.requires_grad = False\n",
    "lidar_cnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b5415-e9ef-442e-a2fa-f4d58747cd62",
   "metadata": {},
   "source": [
    "### 5.1.2 The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313fe37",
   "metadata": {},
   "source": [
    "Below is the dataset we will be using in this assessment. It is similar to the dataset we used in the first few labs, but please note `self.classes`. Unlike the first lab where we predicted position, in this lab, we will determine whether the RGB or LiDAR we are evaluating contains a `cube` or a `sphere`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1e26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Scales data into [0,1]\n",
    "])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir, start_idx, stop_idx):\n",
    "        self.classes = [\"cubes\", \"spheres\"]\n",
    "        self.root_dir = root_dir\n",
    "        self.rgb = []\n",
    "        self.lidar = []\n",
    "        self.class_idxs = []\n",
    "\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            for idx in range(start_idx, stop_idx):\n",
    "                file_number = \"{:04d}\".format(idx)\n",
    "                rbg_img = Image.open(self.root_dir + class_name + \"/rgb/\" + file_number + \".png\")\n",
    "                rbg_img = img_transforms(rbg_img).to(device)\n",
    "                self.rgb.append(rbg_img)\n",
    "    \n",
    "                lidar_depth = np.load(self.root_dir + class_name + \"/lidar/\" + file_number + \".npy\")\n",
    "                lidar_depth = torch.from_numpy(lidar_depth[None, :, :]).to(torch.float32).to(device)\n",
    "                self.lidar.append(lidar_depth)\n",
    "\n",
    "                self.class_idxs.append(torch.tensor(class_idx, dtype=torch.float32)[None].to(device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rbg_img = self.rgb[idx]\n",
    "        lidar_depth = self.lidar[idx]\n",
    "        class_idx = self.class_idxs[idx]\n",
    "        return rbg_img, lidar_depth, class_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534d446-2f2c-4c44-a254-a3f34492685f",
   "metadata": {},
   "source": [
    "This data is available in the `/data/assessment` folder. Here is an example of one of the cubes. The images are small, but there is enough detail that our models will be able to tell the difference.\n",
    "\n",
    "<center><img src=\"data/assessment/cubes/rgb/0002.png\" /></center>\n",
    "\n",
    "Let's go ahead and load the data into a `DataLoader`. We'll set aside a few batches (`VALID_BATCHES`) for validation. The rest of the data will be used for training. We have `9999` images for each of the cube and sphere categories, so we'll multiply N times 2 to reflect the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49e3ec1-7598-44bf-8872-b130eb90b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "VALID_BATCHES = 10\n",
    "N = 9999\n",
    "\n",
    "valid_N = VALID_BATCHES*BATCH_SIZE\n",
    "train_N = N - valid_N\n",
    "\n",
    "train_data = MyDataset(\"data/assessment/\", 0, train_N)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_data = MyDataset(\"data/assessment/\", train_N, N)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "N *= 2\n",
    "valid_N *= 2\n",
    "train_N *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ea94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B Configuration - Change this for different tasks\n",
    "TASK_NAME = \"baseline\"  # Change to \"task1\", \"task2\", \"task3\", etc.\n",
    "WANDB_TAGS = [\"baseline\", \"contrastive\"]  # Add descriptive tags for filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d3353",
   "metadata": {},
   "source": [
    "## 5.2 Contrastive Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541da32",
   "metadata": {},
   "source": [
    "Before we create a cross-modal projection model, it would be nice to have a way to embed our RGB images as a starting point. Let's be efficient with our data and create a contrastive pre-training model. First, it would help to have a convolutional model. We've prepared a recommended architecture below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7c61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CILP_EMB_SIZE = 200\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, in_ch, emb_size=CILP_EMB_SIZE):\n",
    "        super().__init__()\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        # Convolution\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 50, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(50, 100, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(100, 200, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(200, 200, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(200 * 4 * 4, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.conv(x)\n",
    "        emb = self.dense_emb(conv)\n",
    "        return F.normalize(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4ad7f",
   "metadata": {},
   "source": [
    "The RGB data has `4` channels, and our LiDAR data has `1`. Let's initiate these embedding models respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab4b81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embedder = Embedder(4).to(device)\n",
    "lidar_embedder = Embedder(1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4247201",
   "metadata": {},
   "source": [
    "Now that we have our embedding models, let's combine them into a `ContrastivePretraining` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c09a3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastivePretraining(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.img_embedder = img_embedder\n",
    "        self.lidar_embedder = lidar_embedder\n",
    "        self.cos = nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, rgb_imgs, lidar_depths):\n",
    "        img_emb = self.img_embedder(rgb_imgs)\n",
    "        lidar_emb = self.lidar_embedder(lidar_depths)\n",
    "\n",
    "        repeated_img_emb = img_emb.repeat_interleave(len(img_emb), dim=0)\n",
    "        repeated_lidar_emb = lidar_emb.repeat(len(lidar_emb), 1)\n",
    "\n",
    "        similarity = self.cos(repeated_img_emb, repeated_lidar_emb)\n",
    "        similarity = torch.unflatten(similarity, 0, (BATCH_SIZE, BATCH_SIZE))\n",
    "        similarity = (similarity + 1) / 2\n",
    "\n",
    "        logits_per_img = similarity\n",
    "        logits_per_lidar = similarity.T\n",
    "        return logits_per_img, logits_per_lidar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac62782",
   "metadata": {},
   "source": [
    "Time to put these models to the test! First, let's initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a4a6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sc/home/josef.pribbernow/HPI-Applied-Hands-On-Computer-Vision-2025-Josef-Pribbernow/Assignment-02/wandb/run-20251229_184027-58g29j8z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/58g29j8z' target=\"_blank\">CILP_contrastive_pretraining</a></strong> to <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/58g29j8z' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/58g29j8z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/58g29j8z?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x74f21c1fa510>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CILP_LR = 0.0001\n",
    "CILP_model = ContrastivePretraining().to(device)\n",
    "optimizer = torch.optim.AdamW(CILP_model.parameters(), lr=CILP_LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_lidar = nn.CrossEntropyLoss()\n",
    "ground_truth = torch.arange(BATCH_SIZE, dtype=torch.long).to(device)\n",
    "epochs = 3\n",
    "\n",
    "# Initialize W&B for CILP training\n",
    "wandb.init(\n",
    "    project=\"cilp-extended-assessment\",\n",
    "    group=TASK_NAME,\n",
    "    name=\"CILP_contrastive_pretraining\",\n",
    "    tags=WANDB_TAGS + [\"contrastive_pretraining\"],\n",
    "    config={\n",
    "        \"learning_rate\": CILP_LR,\n",
    "        \"architecture\": \"CILP_Contrastive\",\n",
    "        \"embedding_size\": CILP_EMB_SIZE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": epochs,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": \"ReduceLROnPlateau\",\n",
    "        \"fusion_strategy\": \"contrastive\",\n",
    "        \"num_params\": sum(p.numel() for p in CILP_model.parameters() if p.requires_grad),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83430633",
   "metadata": {},
   "source": [
    "Before we can train the model, we should define a loss function to guide our model in learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "181b9036-a22f-474d-b629-7a210dcbfab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CILP_loss(batch):\n",
    "    rbg_img, lidar_depth, class_idx = batch\n",
    "    logits_per_img, logits_per_lidar = CILP_model(rbg_img, lidar_depth)\n",
    "    total_loss = (loss_img(logits_per_img, ground_truth) + loss_lidar(logits_per_lidar, ground_truth))/2\n",
    "    return total_loss, logits_per_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182b887-ef15-4cf1-9916-161f8db05a24",
   "metadata": {},
   "source": [
    "Next, it's time to train. If the above `TODO`s were completed correctly, the loss should be under `3.2`. Are the values along the diagional close to `1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4db7204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train Loss: 3.0831294779358416 \n",
      "Similarity:\n",
      "tensor([[0.9956, 0.6243, 0.9475,  ..., 0.7311, 0.4361, 0.8131],\n",
      "        [0.6609, 0.9916, 0.8185,  ..., 0.2307, 0.6002, 0.9576],\n",
      "        [0.9529, 0.7629, 0.9931,  ..., 0.5457, 0.4802, 0.9311],\n",
      "        ...,\n",
      "        [0.7098, 0.2297, 0.5117,  ..., 0.9951, 0.2588, 0.3361],\n",
      "        [0.4438, 0.5700, 0.4727,  ..., 0.3012, 0.9930, 0.4772],\n",
      "        [0.8054, 0.9327, 0.9325,  ..., 0.3436, 0.5187, 0.9980]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.1898251960152075 \n",
      "Similarity:\n",
      "tensor([[0.9929, 0.8187, 0.3802,  ..., 0.4351, 0.5168, 0.9863],\n",
      "        [0.8388, 0.9929, 0.2445,  ..., 0.2492, 0.3178, 0.8504],\n",
      "        [0.3461, 0.2512, 0.9953,  ..., 0.6651, 0.5370, 0.2902],\n",
      "        ...,\n",
      "        [0.4139, 0.2344, 0.6133,  ..., 0.9965, 0.9845, 0.4190],\n",
      "        [0.4982, 0.2953, 0.5056,  ..., 0.9711, 0.9959, 0.5141],\n",
      "        [0.9864, 0.8342, 0.2877,  ..., 0.4280, 0.5314, 0.9947]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 1\n",
      "Train Loss: 3.0284707799282042 \n",
      "Similarity:\n",
      "tensor([[0.9941, 0.8886, 0.7007,  ..., 0.7846, 0.8294, 0.3135],\n",
      "        [0.9147, 0.9990, 0.4796,  ..., 0.5556, 0.9431, 0.3689],\n",
      "        [0.6732, 0.4936, 0.9975,  ..., 0.9820, 0.5912, 0.2089],\n",
      "        ...,\n",
      "        [0.7610, 0.5668, 0.9850,  ..., 0.9967, 0.6319, 0.2427],\n",
      "        [0.8300, 0.9224, 0.5789,  ..., 0.6337, 0.9968, 0.3250],\n",
      "        [0.3223, 0.3822, 0.2135,  ..., 0.2596, 0.3301, 0.9983]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.1806314367997017 \n",
      "Similarity:\n",
      "tensor([[0.9962, 0.7360, 0.4300,  ..., 0.3600, 0.4298, 0.9767],\n",
      "        [0.7501, 0.9977, 0.3684,  ..., 0.2472, 0.2614, 0.7526],\n",
      "        [0.3974, 0.3677, 0.9979,  ..., 0.5595, 0.4271, 0.3028],\n",
      "        ...,\n",
      "        [0.3419, 0.2305, 0.5879,  ..., 0.9993, 0.9755, 0.3482],\n",
      "        [0.4117, 0.2448, 0.4633,  ..., 0.9786, 0.9985, 0.4390],\n",
      "        [0.9837, 0.7553, 0.3006,  ..., 0.3552, 0.4552, 0.9986]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      "Train Loss: 3.0229348286269713 \n",
      "Similarity:\n",
      "tensor([[0.9780, 0.5506, 0.3817,  ..., 0.3470, 0.3732, 0.2228],\n",
      "        [0.6045, 0.9932, 0.9139,  ..., 0.4815, 0.4541, 0.4469],\n",
      "        [0.4460, 0.9072, 0.9988,  ..., 0.4715, 0.5407, 0.5887],\n",
      "        ...,\n",
      "        [0.3141, 0.4229, 0.4481,  ..., 0.9958, 0.2667, 0.3783],\n",
      "        [0.4041, 0.4608, 0.5447,  ..., 0.2657, 0.9974, 0.6676],\n",
      "        [0.1930, 0.4103, 0.5821,  ..., 0.3857, 0.7137, 0.9967]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.1755214239421643 \n",
      "Similarity:\n",
      "tensor([[0.9982, 0.6552, 0.3553,  ..., 0.3745, 0.4745, 0.9600],\n",
      "        [0.6579, 0.9977, 0.3648,  ..., 0.2973, 0.2995, 0.7496],\n",
      "        [0.3585, 0.3527, 0.9968,  ..., 0.4989, 0.3815, 0.2542],\n",
      "        ...,\n",
      "        [0.3923, 0.2781, 0.4821,  ..., 0.9978, 0.9836, 0.3721],\n",
      "        [0.4785, 0.2793, 0.3812,  ..., 0.9731, 0.9985, 0.4608],\n",
      "        [0.9669, 0.7221, 0.2475,  ..., 0.3620, 0.4679, 0.9986]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cilp_train/loss</td><td>█▂▁</td></tr><tr><td>cilp_valid/loss</td><td>█▄▁</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>learning_rate</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cilp_train/loss</td><td>3.02293</td></tr><tr><td>cilp_valid/loss</td><td>3.17552</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CILP_contrastive_pretraining</strong> at: <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/58g29j8z' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/58g29j8z</a><br> View project at: <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251229_184027-58g29j8z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    CILP_model.train()\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits_per_img = get_CILP_loss(batch)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = train_loss/step\n",
    "    assessment_utils.print_CILP_results(epoch, avg_train_loss, logits_per_img, is_train=True)\n",
    "\n",
    "    CILP_model.eval()\n",
    "    valid_loss = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        loss, logits_per_img = get_CILP_loss(batch)\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    avg_valid_loss = valid_loss/step\n",
    "    assessment_utils.print_CILP_results(epoch, avg_valid_loss, logits_per_img, is_train=False)\n",
    "    \n",
    "    # Step the scheduler based on validation loss\n",
    "    scheduler.step(avg_valid_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"cilp_train/loss\": avg_train_loss,\n",
    "        \"cilp_valid/loss\": avg_valid_loss,\n",
    "        \"learning_rate\": current_lr,\n",
    "        \"epoch\": epoch,\n",
    "    })\n",
    "\n",
    "# Log similarity matrix at end of training as an image\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(logits_per_img.detach().cpu().numpy(), cmap='viridis', aspect='auto')\n",
    "ax.set_xlabel('LiDAR Index')\n",
    "ax.set_ylabel('Image Index')\n",
    "ax.set_title('Similarity Matrix')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "wandb.log({\"similarity_matrix\": wandb.Image(fig)})\n",
    "wandb.finish()\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb8027-4ffb-4ff8-81a2-395f5c76ffcb",
   "metadata": {},
   "source": [
    "When complete, please freeze the model. We will assess this model with our cross-model projection model, and if this model is altered during cross-model projection training, it may not pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea35a356-4536-494b-98c9-806dced9ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in CILP_model.parameters():\n",
    "    CILP_model.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f0fff",
   "metadata": {},
   "source": [
    "The CILP contrastive pre-training has been logged to W&B with:\n",
    "- Training and validation loss per epoch\n",
    "- Learning rate\n",
    "- Similarity matrix visualization at the end of training\n",
    "- All hyperparameters in the config\n",
    "\n",
    "The W&B run has been completed and you can view the results in your W&B dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bf714",
   "metadata": {},
   "source": [
    "## 5.3 Cross-Modal Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674845f5",
   "metadata": {},
   "source": [
    "Now that we have a way to embed our image data, let's move on to cross-modal projection. \n",
    "\n",
    "Let's jump right in and create the projector. What should be the dimensions into the model, and what should be the dimensions out of the model? A hint to the first dimension can be found in section [#5.2-Contrastive-Pre-training](#5.2-Contrastive-Pre-training) in the `Embedder` class. A hint to the second dimension can be found in the [assessment/assesment_utils.py](assessment/assesment_utils.py) file in the `Classifier` class. The dimensions of the output should be the same size as the output of the `get_embs` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6427d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = nn.Sequential(\n",
    "    nn.Linear(CILP_EMB_SIZE, 1000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 3200)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81457189",
   "metadata": {},
   "source": [
    "Next, let's define the loss function for training the `projector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe942748-a017-4725-8350-86cbbbe6e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projector_loss(model, batch):\n",
    "    rbg_img, lidar_depth, class_idx = batch\n",
    "    imb_embs = CILP_model.img_embedder(rbg_img)\n",
    "    lidar_emb = lidar_cnn.get_embs(lidar_depth)\n",
    "    pred_lidar_embs = model(imb_embs)\n",
    "    return nn.MSELoss()(pred_lidar_embs, lidar_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ac873-88ce-4d93-8000-89f77eac929e",
   "metadata": {},
   "source": [
    "The `projector` will take a little while to train, but at the end of it, should reach a validation loss around 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19e75d32-bceb-4e8a-a785-14a9aa4348fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sc/home/josef.pribbernow/HPI-Applied-Hands-On-Computer-Vision-2025-Josef-Pribbernow/Assignment-02/wandb/run-20251229_184041-mmytu8jz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/mmytu8jz' target=\"_blank\">CILP_projector_training</a></strong> to <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/mmytu8jz' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/mmytu8jz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 3.4427\n",
      "Epoch   0 | Valid Loss: 3.1946\n",
      "Epoch   1 | Train Loss: 3.1244\n",
      "Epoch   1 | Valid Loss: 3.1352\n",
      "Epoch   2 | Train Loss: 3.0468\n",
      "Epoch   2 | Valid Loss: 3.1103\n",
      "Epoch   3 | Train Loss: 2.9435\n",
      "Epoch   3 | Valid Loss: 2.9241\n",
      "Epoch   4 | Train Loss: 2.8149\n",
      "Epoch   4 | Valid Loss: 2.8579\n",
      "Epoch   5 | Train Loss: 2.7117\n",
      "Epoch   5 | Valid Loss: 2.7991\n",
      "Epoch   6 | Train Loss: 2.6023\n",
      "Epoch   6 | Valid Loss: 2.6085\n",
      "Epoch   7 | Train Loss: 2.4950\n",
      "Epoch   7 | Valid Loss: 2.5086\n",
      "Epoch   8 | Train Loss: 2.3852\n",
      "Epoch   8 | Valid Loss: 2.4043\n",
      "Epoch   9 | Train Loss: 2.2837\n",
      "Epoch   9 | Valid Loss: 2.3598\n",
      "Epoch  10 | Train Loss: 2.2386\n",
      "Epoch  10 | Valid Loss: 2.3394\n",
      "Epoch  11 | Train Loss: 2.1737\n",
      "Epoch  11 | Valid Loss: 2.2267\n",
      "Epoch  12 | Train Loss: 2.1289\n",
      "Epoch  12 | Valid Loss: 2.1669\n",
      "Epoch  13 | Train Loss: 2.0826\n",
      "Epoch  13 | Valid Loss: 2.2585\n",
      "Epoch  14 | Train Loss: 2.0484\n",
      "Epoch  14 | Valid Loss: 2.3193\n",
      "Epoch  15 | Train Loss: 1.9056\n",
      "Epoch  15 | Valid Loss: 2.1188\n",
      "Epoch  16 | Train Loss: 1.8682\n",
      "Epoch  16 | Valid Loss: 2.0688\n",
      "Epoch  17 | Train Loss: 1.8530\n",
      "Epoch  17 | Valid Loss: 2.0135\n",
      "Epoch  18 | Train Loss: 1.8369\n",
      "Epoch  18 | Valid Loss: 2.0086\n",
      "Epoch  19 | Train Loss: 1.8232\n",
      "Epoch  19 | Valid Loss: 2.0208\n",
      "Epoch  20 | Train Loss: 1.8046\n",
      "Epoch  20 | Valid Loss: 1.9863\n",
      "Epoch  21 | Train Loss: 1.7846\n",
      "Epoch  21 | Valid Loss: 1.9564\n",
      "Epoch  22 | Train Loss: 1.7723\n",
      "Epoch  22 | Valid Loss: 2.0295\n",
      "Epoch  23 | Train Loss: 1.7641\n",
      "Epoch  23 | Valid Loss: 1.9625\n",
      "Epoch  24 | Train Loss: 1.7001\n",
      "Epoch  24 | Valid Loss: 1.8968\n",
      "Epoch  25 | Train Loss: 1.6855\n",
      "Epoch  25 | Valid Loss: 1.8697\n",
      "Epoch  26 | Train Loss: 1.6789\n",
      "Epoch  26 | Valid Loss: 1.8523\n",
      "Epoch  27 | Train Loss: 1.6728\n",
      "Epoch  27 | Valid Loss: 1.8894\n",
      "Epoch  28 | Train Loss: 1.6656\n",
      "Epoch  28 | Valid Loss: 1.8448\n",
      "Epoch  29 | Train Loss: 1.6569\n",
      "Epoch  29 | Valid Loss: 1.8492\n",
      "Epoch  30 | Train Loss: 1.6579\n",
      "Epoch  30 | Valid Loss: 1.8327\n",
      "Epoch  31 | Train Loss: 1.6455\n",
      "Epoch  31 | Valid Loss: 1.8563\n",
      "Epoch  32 | Train Loss: 1.6465\n",
      "Epoch  32 | Valid Loss: 1.8401\n",
      "Epoch  33 | Train Loss: 1.6058\n",
      "Epoch  33 | Valid Loss: 1.8110\n",
      "Epoch  34 | Train Loss: 1.6039\n",
      "Epoch  34 | Valid Loss: 1.8132\n",
      "Epoch  35 | Train Loss: 1.5983\n",
      "Epoch  35 | Valid Loss: 1.8138\n",
      "Epoch  36 | Train Loss: 1.5833\n",
      "Epoch  36 | Valid Loss: 1.7955\n",
      "Epoch  37 | Train Loss: 1.5792\n",
      "Epoch  37 | Valid Loss: 1.7887\n",
      "Epoch  38 | Train Loss: 1.5747\n",
      "Epoch  38 | Valid Loss: 1.8134\n",
      "Epoch  39 | Train Loss: 1.5733\n",
      "Epoch  39 | Valid Loss: 1.7990\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>██████████████▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid/loss</td><td>███▇▆▆▅▅▄▄▄▃▃▃▄▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>1.57325</td></tr><tr><td>valid/loss</td><td>1.799</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CILP_projector_training</strong> at: <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/mmytu8jz' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/mmytu8jz</a><br> View project at: <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251229_184041-mmytu8jz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 40\n",
    "optimizer = torch.optim.AdamW(projector.parameters())\n",
    "assessment_utils.train_model(\n",
    "    projector, \n",
    "    optimizer, \n",
    "    get_projector_loss, \n",
    "    epochs, \n",
    "    train_dataloader, \n",
    "    valid_dataloader,\n",
    "    wandb_project=\"cilp-extended-assessment\",\n",
    "    wandb_name=\"CILP_projector_training\",\n",
    "    wandb_config={\n",
    "        \"architecture\": \"Projector\",\n",
    "        \"group\": TASK_NAME,\n",
    "        \"tags\": WANDB_TAGS + [\"projector_training\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b916d-8bcf-43f0-8c06-2d12302fef05",
   "metadata": {},
   "source": [
    "Time to bring it together. Let's create a new model `RGB2LiDARClassifier` where we can use our projector with the pre-trained `lidar_cnn` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce38f00d-70a5-4d07-af19-58afebb49063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGB2LiDARClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.projector = projector\n",
    "        self.img_embedder = CILP_model.img_embedder\n",
    "        self.shape_classifier = lidar_cnn\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        img_encodings = self.img_embedder(imgs)\n",
    "        proj_lidar_embs = self.projector(img_encodings)\n",
    "        return self.shape_classifier(data_embs=proj_lidar_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9978420f-2887-4b38-82c0-b3888541d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = RGB2LiDARClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3fd01-8364-4165-b344-a48c286dbe92",
   "metadata": {},
   "source": [
    "Before we train this model, let's see how it does out of the box. We'll create a function `get_correct` that we can use to calculate the number of classifications that were correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f470060-3e8e-4b43-b56e-b74fd0160c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct(output, y):\n",
    "    zero_tensor = torch.tensor([0]).to(device)\n",
    "    pred = torch.gt(output, zero_tensor)\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69ad75-fe12-43de-9758-94047cfa3a3d",
   "metadata": {},
   "source": [
    "Next, we can make a `get_valid_metrics` function to calculate the model's accuracy with the validation dataset. If done correctly, the accuracy should be above `.70`, or 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d50b467c-7a11-492b-aa69-bb3695b27e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.1538 | Accuracy 0.8375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1538238167762755, 0.8375)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_valid_metrics():\n",
    "    my_classifier.eval()\n",
    "    correct = 0\n",
    "    batch_correct = 0\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        rbg_img, _, class_idx = batch\n",
    "        output = my_classifier(rbg_img)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n",
    "        batch_correct = get_correct(output, class_idx)\n",
    "        correct += batch_correct\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / (step + 1)\n",
    "    accuracy = correct / valid_N\n",
    "    print(f\"Valid Loss: {avg_loss:2.4f} | Accuracy {accuracy:2.4f}\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "get_valid_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85538953-0ee3-4168-898e-baaa8f8c30e6",
   "metadata": {},
   "source": [
    "Finally, let's fine-tune the completed model. Since `CILP` and `lidar_cnn` are frozen, this should only change the `projector` part of the model. Even so, the model should achieve a validation accuracy of above `.95` or 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10dd4664-7520-47a2-92d0-b455d9678f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sc/home/josef.pribbernow/HPI-Applied-Hands-On-Computer-Vision-2025-Josef-Pribbernow/Assignment-02/wandb/run-20251229_184235-lnvmip1y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/lnvmip1y' target=\"_blank\">CILP_final_classifier</a></strong> to <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/lnvmip1y' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/lnvmip1y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4093 | Accuracy 0.7742\n",
      "Valid Loss: 0.1002 | Accuracy 0.9609\n",
      "Train Loss: 0.0396 | Accuracy 0.9866\n",
      "Valid Loss: 0.0015 | Accuracy 1.0000\n",
      "Train Loss: 0.0136 | Accuracy 0.9944\n",
      "Valid Loss: 0.0004 | Accuracy 1.0000\n",
      "Train Loss: 0.0157 | Accuracy 0.9934\n",
      "Valid Loss: 0.0005 | Accuracy 1.0000\n",
      "Train Loss: 0.0091 | Accuracy 0.9952\n",
      "Valid Loss: 0.0012 | Accuracy 0.9984\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "CLASSIFIER_LR = 0.001\n",
    "optimizer = torch.optim.AdamW(my_classifier.parameters(), lr=CLASSIFIER_LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"cilp-extended-assessment\",\n",
    "    group=TASK_NAME,\n",
    "    name=\"CILP_final_classifier\",\n",
    "    tags=WANDB_TAGS + [\"final_classifier\"],\n",
    "    config={\n",
    "        \"learning_rate\": CLASSIFIER_LR,\n",
    "        \"architecture\": \"RGB2LiDARClassifier\",\n",
    "        \"embedding_size\": CILP_EMB_SIZE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": epochs,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": \"ReduceLROnPlateau\",\n",
    "        \"fusion_strategy\": \"contrastive\",\n",
    "        \"num_params\": sum(p.numel() for p in my_classifier.parameters() if p.requires_grad),\n",
    "    }\n",
    ")\n",
    "\n",
    "my_classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    batch_correct = 0\n",
    "    total_train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        rbg_img, _, class_idx = batch\n",
    "        output = my_classifier(rbg_img)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n",
    "        batch_correct = get_correct(output, class_idx)\n",
    "        correct += batch_correct\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / (step + 1)\n",
    "    train_accuracy = correct / train_N\n",
    "    print(f\"Train Loss: {avg_train_loss:2.4f} | Accuracy {train_accuracy:2.4f}\")\n",
    "    valid_loss, valid_acc = get_valid_metrics()\n",
    "    \n",
    "    # Step the scheduler based on validation loss\n",
    "    scheduler.step(valid_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    wandb.log({\n",
    "        \"train/loss\": avg_train_loss,\n",
    "        \"train/accuracy\": train_accuracy,\n",
    "        \"valid/loss\": valid_loss,\n",
    "        \"valid/accuracy\": valid_acc,\n",
    "        \"learning_rate\": current_lr,\n",
    "\n",
    "        \"epoch\": epoch,    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8b58e",
   "metadata": {},
   "source": [
    "Sample 5 predictions and log them to Weights & Biases (wandb) for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86eb392b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>learning_rate</td><td>████▁</td></tr><tr><td>train/accuracy</td><td>▁████</td></tr><tr><td>train/loss</td><td>█▂▁▁▁</td></tr><tr><td>valid/accuracy</td><td>▁████</td></tr><tr><td>valid/loss</td><td>█▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>learning_rate</td><td>0.0005</td></tr><tr><td>train/accuracy</td><td>0.9952</td></tr><tr><td>train/loss</td><td>0.00906</td></tr><tr><td>valid/accuracy</td><td>0.99844</td></tr><tr><td>valid/loss</td><td>0.00121</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CILP_final_classifier</strong> at: <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/lnvmip1y' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/lnvmip1y</a><br> View project at: <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment</a><br>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251229_184235-lnvmip1y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_classifier.eval()\n",
    "for step, batch in enumerate(valid_dataloader):\n",
    "    rbg_img, _, class_idx = batch\n",
    "    output = my_classifier(rbg_img)\n",
    "    wandb.log({\"predictions\": wandb.Table(data=[[rbg_img[i].cpu().numpy(), torch.sigmoid(output[i]).item(), class_idx[i].item()] for i in range(5)],\n",
    "                                           columns=[\"rgb_image\", \"predicted_class\", \"true_class\"])})\n",
    "    break\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahocv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

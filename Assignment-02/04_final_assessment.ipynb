{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e60ab08",
   "metadata": {},
   "source": [
    "# Task 5: CILP Assessment Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1083571",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c241ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjosefpribbernow\u001b[0m (\u001b[33mjosefpribbernow-hasso-plattner-institute\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from assessment import assessment_utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30f447-41fd-4b60-9e11-d325ca379bb7",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cf3584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (embedder): Sequential(\n",
       "    (0): Conv2d(1, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=3200, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lidar_cnn = assessment_utils.Classifier(1).to(device)\n",
    "lidar_cnn.load_state_dict(torch.load(\"assessment/lidar_cnn.pt\", weights_only=True))\n",
    "# Do not unfreeze. Otherwise, it would be difficult to pass the assessment.\n",
    "for param in lidar_cnn.parameters():\n",
    "    lidar_cnn.requires_grad = False\n",
    "lidar_cnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beaafd3",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1e26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    # transforms.ToImage(),\n",
    "    # transforms.ToDtype(torch.float32, scale=True),  # Scales data into [0,1]\n",
    "    transforms.ToTensor(),  # Using ToTensor for compatibility, as ToImage is not available, in my fucked up environment\n",
    "])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir, start_idx, stop_idx):\n",
    "        self.classes = [\"cubes\", \"spheres\"]\n",
    "        self.root_dir = root_dir\n",
    "        self.rgb = []\n",
    "        self.lidar = []\n",
    "        self.class_idxs = []\n",
    "\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            for idx in range(start_idx, stop_idx):\n",
    "                file_number = \"{:04d}\".format(idx)\n",
    "                rbg_img = Image.open(self.root_dir + class_name + \"/rgb/\" + file_number + \".png\")\n",
    "                rbg_img = img_transforms(rbg_img).to(device)\n",
    "                self.rgb.append(rbg_img)\n",
    "    \n",
    "                lidar_depth = np.load(self.root_dir + class_name + \"/lidar/\" + file_number + \".npy\")\n",
    "                lidar_depth = torch.from_numpy(lidar_depth[None, :, :]).to(torch.float32).to(device)\n",
    "                self.lidar.append(lidar_depth)\n",
    "\n",
    "                self.class_idxs.append(torch.tensor(class_idx, dtype=torch.float32)[None].to(device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rbg_img = self.rgb[idx]\n",
    "        lidar_depth = self.lidar[idx]\n",
    "        class_idx = self.class_idxs[idx]\n",
    "        return rbg_img, lidar_depth, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49e3ec1-7598-44bf-8872-b130eb90b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "VALID_BATCHES = 10\n",
    "N = 9999\n",
    "\n",
    "valid_N = VALID_BATCHES*BATCH_SIZE\n",
    "train_N = N - valid_N\n",
    "\n",
    "train_data = MyDataset(\"data/assessment/\", 0, train_N)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_data = MyDataset(\"data/assessment/\", train_N, N)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "N *= 2\n",
    "valid_N *= 2\n",
    "train_N *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ea94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B Configuration\n",
    "TASK_NAME = \"Final Assessment\"\n",
    "WANDB_TAGS = [\"Final Assessment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d3353",
   "metadata": {},
   "source": [
    "## 5.1 Contrastive Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4ff5b",
   "metadata": {},
   "source": [
    "**Decisions:**\n",
    "We use the provided Embedder archtecture from 05_Assessment.ipynb. Also we use MaxPool2d as downsampling method, as it performed best in the ablation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7c61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CILP_EMB_SIZE = 200\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, in_ch, emb_size=CILP_EMB_SIZE):\n",
    "        super().__init__()\n",
    "        kernel_size = 3\n",
    "\n",
    "        # Convolution\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 50, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(50, 100, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(100, 200, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(200, 200, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(200 * 4 * 4, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.conv(x)\n",
    "        emb = self.dense_emb(conv)\n",
    "        return F.normalize(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab4b81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embedder = Embedder(4).to(device)\n",
    "lidar_embedder = Embedder(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c09a3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastivePretraining(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.img_embedder = img_embedder\n",
    "        self.lidar_embedder = lidar_embedder\n",
    "        self.cos = nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, rgb_imgs, lidar_depths):\n",
    "        img_emb = self.img_embedder(rgb_imgs)\n",
    "        lidar_emb = self.lidar_embedder(lidar_depths)\n",
    "\n",
    "        repeated_img_emb = img_emb.repeat_interleave(len(img_emb), dim=0)\n",
    "        repeated_lidar_emb = lidar_emb.repeat(len(lidar_emb), 1)\n",
    "\n",
    "        similarity = self.cos(repeated_img_emb, repeated_lidar_emb)\n",
    "        similarity = torch.unflatten(similarity, 0, (BATCH_SIZE, BATCH_SIZE))\n",
    "        similarity = (similarity + 1) / 2\n",
    "\n",
    "        logits_per_img = similarity\n",
    "        logits_per_lidar = similarity.T\n",
    "        return logits_per_img, logits_per_lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a4a6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sc/home/josef.pribbernow/HPI-Applied-Hands-On-Computer-Vision-2025-Josef-Pribbernow/Assignment-02/wandb/run-20251230_183356-el3wluj9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/el3wluj9' target=\"_blank\">04_contrastive_pretraining</a></strong> to <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/el3wluj9' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/el3wluj9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/el3wluj9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x79361c2a9790>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CILP_LR = 0.0001\n",
    "CILP_model = ContrastivePretraining().to(device)\n",
    "optimizer = torch.optim.AdamW(CILP_model.parameters(), lr=CILP_LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_lidar = nn.CrossEntropyLoss()\n",
    "ground_truth = torch.arange(BATCH_SIZE, dtype=torch.long).to(device)\n",
    "epochs = 3\n",
    "\n",
    "# Initialize W&B for CILP training\n",
    "wandb.init(\n",
    "    project=\"cilp-extended-assessment\",\n",
    "    group=TASK_NAME,\n",
    "    name=\"04_contrastive_pretraining\",\n",
    "    tags=WANDB_TAGS + [\"contrastive_pretraining\"],\n",
    "    config={\n",
    "        \"learning_rate\": CILP_LR,\n",
    "        \"architecture\": \"CILP_Contrastive\",\n",
    "        \"embedding_size\": CILP_EMB_SIZE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": epochs,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": \"ReduceLROnPlateau\",\n",
    "        \"fusion_strategy\": \"contrastive\",\n",
    "        \"num_params\": sum(p.numel() for p in CILP_model.parameters() if p.requires_grad),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "181b9036-a22f-474d-b629-7a210dcbfab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CILP_loss(batch):\n",
    "    rbg_img, lidar_depth, class_idx = batch\n",
    "    logits_per_img, logits_per_lidar = CILP_model(rbg_img, lidar_depth)\n",
    "    total_loss = (loss_img(logits_per_img, ground_truth) + loss_lidar(logits_per_lidar, ground_truth))/2\n",
    "    return total_loss, logits_per_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4db7204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train Loss: 3.0853904412555853 \n",
      "Similarity:\n",
      "tensor([[0.9976, 0.6505, 0.4310,  ..., 0.8315, 0.4353, 0.3540],\n",
      "        [0.6577, 0.9971, 0.0366,  ..., 0.9429, 0.1506, 0.8389],\n",
      "        [0.4685, 0.0447, 0.9972,  ..., 0.1142, 0.8984, 0.1690],\n",
      "        ...,\n",
      "        [0.8833, 0.8906, 0.1490,  ..., 0.9892, 0.1719, 0.5702],\n",
      "        [0.3745, 0.1390, 0.9145,  ..., 0.0976, 0.9934, 0.4095],\n",
      "        [0.3605, 0.8644, 0.1730,  ..., 0.6582, 0.3792, 0.9982]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.192580499147114 \n",
      "Similarity:\n",
      "tensor([[0.9942, 0.7943, 0.5059,  ..., 0.4865, 0.4968, 0.9824],\n",
      "        [0.8727, 0.9938, 0.2315,  ..., 0.2860, 0.3188, 0.9130],\n",
      "        [0.4441, 0.1624, 0.9937,  ..., 0.6094, 0.5524, 0.3754],\n",
      "        ...,\n",
      "        [0.4157, 0.2911, 0.5634,  ..., 0.9985, 0.9942, 0.3972],\n",
      "        [0.4508, 0.3343, 0.5197,  ..., 0.9956, 0.9979, 0.4372],\n",
      "        [0.9952, 0.8599, 0.4179,  ..., 0.4431, 0.4631, 0.9971]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 1\n",
      "Train Loss: 3.03291774626395 \n",
      "Similarity:\n",
      "tensor([[0.9973, 0.2363, 0.3848,  ..., 0.7058, 0.3806, 0.9456],\n",
      "        [0.2637, 0.9951, 0.7389,  ..., 0.0633, 0.3468, 0.3975],\n",
      "        [0.3814, 0.7148, 0.9984,  ..., 0.4827, 0.2986, 0.3184],\n",
      "        ...,\n",
      "        [0.6649, 0.0542, 0.4892,  ..., 0.9979, 0.5704, 0.4750],\n",
      "        [0.3746, 0.3493, 0.2910,  ..., 0.5460, 0.9990, 0.3286],\n",
      "        [0.9450, 0.3888, 0.2911,  ..., 0.4980, 0.3347, 0.9980]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.183976123207494 \n",
      "Similarity:\n",
      "tensor([[0.9981, 0.6759, 0.3434,  ..., 0.3850, 0.4438, 0.9886],\n",
      "        [0.6952, 0.9978, 0.3735,  ..., 0.2344, 0.2356, 0.6965],\n",
      "        [0.3408, 0.3454, 0.9977,  ..., 0.5982, 0.5047, 0.2672],\n",
      "        ...,\n",
      "        [0.3602, 0.2433, 0.5928,  ..., 0.9992, 0.9869, 0.3547],\n",
      "        [0.4356, 0.2471, 0.4918,  ..., 0.9852, 0.9983, 0.4427],\n",
      "        [0.9923, 0.6855, 0.2573,  ..., 0.3676, 0.4413, 0.9988]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      "Train Loss: 3.026665212700814 \n",
      "Similarity:\n",
      "tensor([[0.9992, 0.3143, 0.3313,  ..., 0.9984, 0.6565, 0.3642],\n",
      "        [0.2930, 0.9978, 0.9803,  ..., 0.3125, 0.7175, 0.5438],\n",
      "        [0.3058, 0.9918, 0.9978,  ..., 0.3313, 0.6863, 0.6393],\n",
      "        ...,\n",
      "        [0.9966, 0.3486, 0.3695,  ..., 0.9986, 0.6730, 0.3835],\n",
      "        [0.6172, 0.7222, 0.6878,  ..., 0.6172, 0.9970, 0.2072],\n",
      "        [0.3967, 0.5850, 0.6731,  ..., 0.4259, 0.2301, 0.9973]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.1817332694405005 \n",
      "Similarity:\n",
      "tensor([[0.9980, 0.7222, 0.3088,  ..., 0.4260, 0.5065, 0.9870],\n",
      "        [0.7237, 0.9987, 0.3910,  ..., 0.3033, 0.3087, 0.7075],\n",
      "        [0.2723, 0.3646, 0.9982,  ..., 0.5696, 0.4670, 0.2120],\n",
      "        ...,\n",
      "        [0.4031, 0.3074, 0.5785,  ..., 0.9994, 0.9839, 0.3941],\n",
      "        [0.4911, 0.3103, 0.4802,  ..., 0.9855, 0.9990, 0.4927],\n",
      "        [0.9940, 0.7130, 0.2202,  ..., 0.4065, 0.5030, 0.9993]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cilp_train/loss</td><td>█▂▁</td></tr><tr><td>cilp_valid/loss</td><td>█▂▁</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>learning_rate</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cilp_train/loss</td><td>3.02667</td></tr><tr><td>cilp_valid/loss</td><td>3.18173</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">04_contrastive_pretraining</strong> at: <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/el3wluj9' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/el3wluj9</a><br> View project at: <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251230_183356-el3wluj9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    CILP_model.train()\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits_per_img = get_CILP_loss(batch)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = train_loss/step\n",
    "    assessment_utils.print_CILP_results(epoch, avg_train_loss, logits_per_img, is_train=True)\n",
    "\n",
    "    CILP_model.eval()\n",
    "    valid_loss = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        loss, logits_per_img = get_CILP_loss(batch)\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    avg_valid_loss = valid_loss/step\n",
    "    assessment_utils.print_CILP_results(epoch, avg_valid_loss, logits_per_img, is_train=False)\n",
    "    \n",
    "    # Step the scheduler based on validation loss\n",
    "    scheduler.step(avg_valid_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"cilp_train/loss\": avg_train_loss,\n",
    "        \"cilp_valid/loss\": avg_valid_loss,\n",
    "        \"learning_rate\": current_lr,\n",
    "        \"epoch\": epoch,\n",
    "    })\n",
    "\n",
    "# Log similarity matrix at end of training as an image\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(logits_per_img.detach().cpu().numpy(), cmap='viridis', aspect='auto')\n",
    "ax.set_xlabel('LiDAR Index')\n",
    "ax.set_ylabel('Image Index')\n",
    "ax.set_title('Similarity Matrix')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "wandb.log({\"similarity_matrix\": wandb.Image(fig)})\n",
    "wandb.finish()\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa2c492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CILP model to models/04_contrastive_pretraining.pth\n"
     ]
    }
   ],
   "source": [
    "# Save CILP model checkpoint\n",
    "torch.save(CILP_model.state_dict(), \"models/04_contrastive_pretraining.pth\")\n",
    "print(\"Saved CILP model to models/04_contrastive_pretraining.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea35a356-4536-494b-98c9-806dced9ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in CILP_model.parameters():\n",
    "    CILP_model.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bf714",
   "metadata": {},
   "source": [
    "## 5.2 Cross-Modal Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6427d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = nn.Sequential(\n",
    "    nn.Linear(CILP_EMB_SIZE, 1000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 3200)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe942748-a017-4725-8350-86cbbbe6e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projector_loss(model, batch):\n",
    "    rbg_img, lidar_depth, class_idx = batch\n",
    "    imb_embs = CILP_model.img_embedder(rbg_img)\n",
    "    lidar_emb = lidar_cnn.get_embs(lidar_depth)\n",
    "    pred_lidar_embs = model(imb_embs)\n",
    "    return nn.MSELoss()(pred_lidar_embs, lidar_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19e75d32-bceb-4e8a-a785-14a9aa4348fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sc/home/josef.pribbernow/HPI-Applied-Hands-On-Computer-Vision-2025-Josef-Pribbernow/Assignment-02/wandb/run-20251230_183412-ff5ruz67</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/ff5ruz67' target=\"_blank\">04_projector_training</a></strong> to <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/ff5ruz67' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/ff5ruz67</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 3.4974\n",
      "Epoch   0 | Valid Loss: 3.2700\n",
      "Epoch   1 | Train Loss: 3.1812\n",
      "Epoch   1 | Valid Loss: 3.2089\n",
      "Epoch   2 | Train Loss: 3.1319\n",
      "Epoch   2 | Valid Loss: 3.1251\n",
      "Epoch   3 | Train Loss: 3.0906\n",
      "Epoch   3 | Valid Loss: 3.1192\n",
      "Epoch   4 | Train Loss: 3.0602\n",
      "Epoch   4 | Valid Loss: 3.0859\n",
      "Epoch   5 | Train Loss: 3.0167\n",
      "Epoch   5 | Valid Loss: 3.0149\n",
      "Epoch   6 | Train Loss: 2.9154\n",
      "Epoch   6 | Valid Loss: 2.9643\n",
      "Epoch   7 | Train Loss: 2.7895\n",
      "Epoch   7 | Valid Loss: 2.7400\n",
      "Epoch   8 | Train Loss: 2.6158\n",
      "Epoch   8 | Valid Loss: 2.7514\n",
      "Epoch   9 | Train Loss: 2.5252\n",
      "Epoch   9 | Valid Loss: 2.7436\n",
      "Epoch  10 | Train Loss: 2.3127\n",
      "Epoch  10 | Valid Loss: 2.3582\n",
      "Epoch  11 | Train Loss: 2.2281\n",
      "Epoch  11 | Valid Loss: 2.3297\n",
      "Epoch  12 | Train Loss: 2.1649\n",
      "Epoch  12 | Valid Loss: 2.3392\n",
      "Epoch  13 | Train Loss: 2.1342\n",
      "Epoch  13 | Valid Loss: 2.3059\n",
      "Epoch  14 | Train Loss: 2.0964\n",
      "Epoch  14 | Valid Loss: 2.2836\n",
      "Epoch  15 | Train Loss: 2.0746\n",
      "Epoch  15 | Valid Loss: 2.2377\n",
      "Epoch  16 | Train Loss: 2.0552\n",
      "Epoch  16 | Valid Loss: 2.1984\n",
      "Epoch  17 | Train Loss: 2.0235\n",
      "Epoch  17 | Valid Loss: 2.2369\n",
      "Epoch  18 | Train Loss: 2.0127\n",
      "Epoch  18 | Valid Loss: 2.1756\n",
      "Epoch  19 | Train Loss: 2.0029\n",
      "Epoch  19 | Valid Loss: 2.1686\n",
      "Epoch  20 | Train Loss: 1.9802\n",
      "Epoch  20 | Valid Loss: 2.0918\n",
      "Epoch  21 | Train Loss: 1.9620\n",
      "Epoch  21 | Valid Loss: 2.1459\n",
      "Epoch  22 | Train Loss: 1.9621\n",
      "Epoch  22 | Valid Loss: 2.1361\n",
      "Epoch  23 | Train Loss: 1.8799\n",
      "Epoch  23 | Valid Loss: 2.0570\n",
      "Epoch  24 | Train Loss: 1.8663\n",
      "Epoch  24 | Valid Loss: 2.0626\n",
      "Epoch  25 | Train Loss: 1.8599\n",
      "Epoch  25 | Valid Loss: 2.0462\n",
      "Epoch  26 | Train Loss: 1.8541\n",
      "Epoch  26 | Valid Loss: 2.0464\n",
      "Epoch  27 | Train Loss: 1.8501\n",
      "Epoch  27 | Valid Loss: 2.0573\n",
      "Epoch  28 | Train Loss: 1.8135\n",
      "Epoch  28 | Valid Loss: 2.0406\n",
      "Epoch  29 | Train Loss: 1.8057\n",
      "Epoch  29 | Valid Loss: 2.0136\n",
      "Epoch  30 | Train Loss: 1.8007\n",
      "Epoch  30 | Valid Loss: 2.0006\n",
      "Epoch  31 | Train Loss: 1.8048\n",
      "Epoch  31 | Valid Loss: 2.0014\n",
      "Epoch  32 | Train Loss: 1.7939\n",
      "Epoch  32 | Valid Loss: 2.0158\n",
      "Epoch  33 | Train Loss: 1.7780\n",
      "Epoch  33 | Valid Loss: 1.9807\n",
      "Epoch  34 | Train Loss: 1.7781\n",
      "Epoch  34 | Valid Loss: 1.9982\n",
      "Epoch  35 | Train Loss: 1.7747\n",
      "Epoch  35 | Valid Loss: 1.9950\n",
      "Epoch  36 | Train Loss: 1.7623\n",
      "Epoch  36 | Valid Loss: 1.9719\n",
      "Epoch  37 | Train Loss: 1.7631\n",
      "Epoch  37 | Valid Loss: 1.9793\n",
      "Epoch  38 | Train Loss: 1.7593\n",
      "Epoch  38 | Valid Loss: 1.9748\n",
      "Epoch  39 | Train Loss: 1.7549\n",
      "Epoch  39 | Valid Loss: 1.9729\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>█████████▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▆▆▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid/loss</td><td>██▇▇▇▇▆▅▅▅▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>1.75495</td></tr><tr><td>valid/loss</td><td>1.97287</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">04_projector_training</strong> at: <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/ff5ruz67' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/ff5ruz67</a><br> View project at: <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251230_183412-ff5ruz67/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 40\n",
    "optimizer = torch.optim.AdamW(projector.parameters())\n",
    "assessment_utils.train_model(\n",
    "    projector, \n",
    "    optimizer, \n",
    "    get_projector_loss, \n",
    "    epochs, \n",
    "    train_dataloader, \n",
    "    valid_dataloader,\n",
    "    wandb_project=\"cilp-extended-assessment\",\n",
    "    wandb_name=\"04_projector_training\",\n",
    "    wandb_config={\n",
    "        \"architecture\": \"Projector\",\n",
    "        \"group\": TASK_NAME,\n",
    "        \"tags\": WANDB_TAGS + [\"projector_training\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca39f233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved projector to models/04_projector_training.pth\n"
     ]
    }
   ],
   "source": [
    "# Save projector checkpoint\n",
    "torch.save(projector.state_dict(), \"models/04_projector_training.pth\")\n",
    "print(\"Saved projector to models/04_projector_training.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74d584",
   "metadata": {},
   "source": [
    "## 5.3 Final Classifier (RGB-to-LiDAR classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce38f00d-70a5-4d07-af19-58afebb49063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGB2LiDARClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.projector = projector\n",
    "        self.img_embedder = CILP_model.img_embedder\n",
    "        self.shape_classifier = lidar_cnn\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        img_encodings = self.img_embedder(imgs)\n",
    "        proj_lidar_embs = self.projector(img_encodings)\n",
    "        return self.shape_classifier(data_embs=proj_lidar_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9978420f-2887-4b38-82c0-b3888541d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = RGB2LiDARClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f470060-3e8e-4b43-b56e-b74fd0160c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct(output, y):\n",
    "    zero_tensor = torch.tensor([0]).to(device)\n",
    "    pred = torch.gt(output, zero_tensor)\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d50b467c-7a11-492b-aa69-bb3695b27e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.6064 | Accuracy 0.7953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.6063885599374772, 0.7953125)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_valid_metrics():\n",
    "    my_classifier.eval()\n",
    "    correct = 0\n",
    "    batch_correct = 0\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        rbg_img, _, class_idx = batch\n",
    "        output = my_classifier(rbg_img)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n",
    "        batch_correct = get_correct(output, class_idx)\n",
    "        correct += batch_correct\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / (step + 1)\n",
    "    accuracy = correct / valid_N\n",
    "    print(f\"Valid Loss: {avg_loss:2.4f} | Accuracy {accuracy:2.4f}\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "get_valid_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10dd4664-7520-47a2-92d0-b455d9678f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sc/home/josef.pribbernow/HPI-Applied-Hands-On-Computer-Vision-2025-Josef-Pribbernow/Assignment-02/wandb/run-20251230_183609-gnqwn8ra</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/gnqwn8ra' target=\"_blank\">04_final_classifier</a></strong> to <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/gnqwn8ra' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/gnqwn8ra</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3383 | Accuracy 0.8544\n",
      "Valid Loss: 0.0105 | Accuracy 0.9953\n",
      "Train Loss: 0.0205 | Accuracy 0.9913\n",
      "Valid Loss: 0.0100 | Accuracy 0.9953\n",
      "Train Loss: 0.0138 | Accuracy 0.9949\n",
      "Valid Loss: 0.1297 | Accuracy 0.9453\n",
      "Train Loss: 0.0211 | Accuracy 0.9930\n",
      "Valid Loss: 0.0022 | Accuracy 0.9984\n",
      "Train Loss: 0.0085 | Accuracy 0.9958\n",
      "Valid Loss: 0.0198 | Accuracy 0.9938\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "CLASSIFIER_LR = 0.001\n",
    "optimizer = torch.optim.AdamW(my_classifier.parameters(), lr=CLASSIFIER_LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"cilp-extended-assessment\",\n",
    "    group=TASK_NAME,\n",
    "    name=\"04_final_classifier\",\n",
    "    tags=WANDB_TAGS + [\"final_classifier\"],\n",
    "    config={\n",
    "        \"learning_rate\": CLASSIFIER_LR,\n",
    "        \"architecture\": \"RGB2LiDARClassifier\",\n",
    "        \"embedding_size\": CILP_EMB_SIZE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": epochs,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": \"ReduceLROnPlateau\",\n",
    "        \"fusion_strategy\": \"contrastive\",\n",
    "        \"num_params\": sum(p.numel() for p in my_classifier.parameters() if p.requires_grad),\n",
    "    }\n",
    ")\n",
    "\n",
    "my_classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    batch_correct = 0\n",
    "    total_train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        rbg_img, _, class_idx = batch\n",
    "        output = my_classifier(rbg_img)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n",
    "        batch_correct = get_correct(output, class_idx)\n",
    "        correct += batch_correct\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / (step + 1)\n",
    "    train_accuracy = correct / train_N\n",
    "    print(f\"Train Loss: {avg_train_loss:2.4f} | Accuracy {train_accuracy:2.4f}\")\n",
    "    valid_loss, valid_acc = get_valid_metrics()\n",
    "    \n",
    "    # Step the scheduler based on validation loss\n",
    "    scheduler.step(valid_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    wandb.log({\n",
    "        \"train/loss\": avg_train_loss,\n",
    "        \"train/accuracy\": train_accuracy,\n",
    "        \"valid/loss\": valid_loss,\n",
    "        \"valid/accuracy\": valid_acc,\n",
    "        \"learning_rate\": current_lr,\n",
    "\n",
    "        \"epoch\": epoch,    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebeb3bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final classifier to models/04_final_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "# Save final classifier checkpoint\n",
    "torch.save(my_classifier.state_dict(), \"models/04_final_classifier.pth\")\n",
    "print(\"Saved final classifier to models/04_final_classifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8b58e",
   "metadata": {},
   "source": [
    "Sample 5 predictions and log them to Weights & Biases (wandb) for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86eb392b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁</td></tr><tr><td>train/accuracy</td><td>▁████</td></tr><tr><td>train/loss</td><td>█▁▁▁▁</td></tr><tr><td>valid/accuracy</td><td>██▁█▇</td></tr><tr><td>valid/loss</td><td>▁▁█▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>train/accuracy</td><td>0.99576</td></tr><tr><td>train/loss</td><td>0.00853</td></tr><tr><td>valid/accuracy</td><td>0.99375</td></tr><tr><td>valid/loss</td><td>0.01985</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">04_final_classifier</strong> at: <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/gnqwn8ra' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment/runs/gnqwn8ra</a><br> View project at: <a href='https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/josefpribbernow-hasso-plattner-institute/cilp-extended-assessment</a><br>Synced 4 W&B file(s), 1 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251230_183609-gnqwn8ra/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_classifier.eval()\n",
    "for step, batch in enumerate(valid_dataloader):\n",
    "    rbg_img, _, class_idx = batch\n",
    "    output = my_classifier(rbg_img)\n",
    "    wandb.log({\"predictions\": wandb.Table(data=[[rbg_img[i].cpu().numpy(), torch.sigmoid(output[i]).item(), class_idx[i].item()] for i in range(5)],\n",
    "                                           columns=[\"rgb_image\", \"predicted_class\", \"true_class\"])})\n",
    "    break\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
